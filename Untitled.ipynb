{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9da80ce-8481-4cac-8eb3-6d0ac6954636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f29392-6d6d-4fdc-987f-2ae722aa0abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704192941c3646b497dedac67440bfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a5a2f3a51a485b97d1d92bd34ef82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54964993809f4b54af34673c347035dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0523215ff0ac446fb88c85f782a37fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf51e48de1a43bd9f4fc41c6f506ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = SummarizationPipeline(\n",
    "    model=AutoModelWithLMHead.from_pretrained(\"SEBIS/code_trans_t5_large_code_documentation_generation_python_multitask_finetune\"),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"SEBIS/code_trans_t5_large_code_documentation_generation_python_multitask_finetune\", skip_special_tokens=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49da58e-54f1-464e-a9d6-c106ad7ec901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Given a link to a website return the source .'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_code = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "pipeline([tokenized_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e0ac278-b29c-417a-bf5f-17906f8cfd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('test-file.py', 'r')\n",
    "file = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4886cbc-db3d-4fa0-a1af-7177e3f6fb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import newspaper\\n\\n\\ndef find_max(values):\\n    return max(values)\\n\\n\\ndef average(values: list) -> int:\\n\\n    total = 0\\n    for i in values:\\n        total += i\\n\\n    return total/len(values)\\n\\n\\ndef chunks(l, n):\\n    d, r = divmod(len(l), n)\\n    for i in range(n):\\n        si = (d + 1) * (i if i < r else r) + d * (0 if i < r else i - r)\\n        yield l[si:si + (d + 1 if i < r else d)]\\n\\n\\ndef pull_articles_from_source(url, source, article_data=[]):\\n    paper = newspaper.build(url)\\n    i = 0\\n    failed = 0\\n    print(len(paper.articles))\\n    paper.download_articles()\\n    paper.parse_articles()  # remove articles that are too small (probably not articles)\\n    print(len(paper.articles))\\n    for article in paper.articles:\\n        i += 1\\n        if i > 10:\\n            break\\n        try:\\n            # fail if the article is empty or less than 40 words\\n            if article.text.isspace() or article.text == '' or len(article.text.split(' ')) < 40:\\n                failed += 1\\n                continue\\n            article.nlp()\\n\\n            authors = article.authors\\n            temp = []\\n            for i in authors:\\n                if len(i.split(' ')) > 5:\\n                    continue\\n                temp.append(i)\\n            authors = temp\\n\\n            data = {'source': source, 'title': article.title, 'authors': authors, 'text': article.text,\\n                    'keywords': article.keywords, 'summary': article.summary, 'url': article.url,\\n                    'date': article.publish_date}\\n            article_data.append(data)\\n        except:\\n            failed += 1\\n\\n    return article_data\\n\\n\\ndef source_from_url(link):\\n\\n    if 'www' in link:\\n        source = link.split('.')[1]\\n    else:\\n        if '.com' in link:\\n            source = link.split('.com')[0]\\n        else:\\n            source = link.split('.')[0]\\n    source = source.replace('https://', '')\\n    source = source.replace('http://', '')\\n    return source\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e90a885-45e3-4ce8-868c-2d27e61fd811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find first def\n",
    "file.find('def')\n",
    "\n",
    "# then grab all of the following text until there is no space between a newline character and the next character (signifiying the first code outside the function)\n",
    "\n",
    "\n",
    "\n",
    "# if not found then run it on the whole thing because that was the end of the file\n",
    "# you don't look for return statements because there can be more than one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85abe596-1741-4be8-b9ba-f19f01d79e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def find_max(values):\\n    return max(values)\\n\\n\\ndef average(values: list) -> int:\\n\\n    total = 0\\n    for i in values:\\n        total += i\\n\\n    return total/len(values)\\n\\n\\ndef chunks(l, n):\\n    d, r = divmod(len(l), n)\\n    for i in range(n):\\n        si = (d + 1) * (i if i < r else r) + d * (0 if i < r else i - r)\\n        yield l[si:si + (d + 1 if i < r else d)]\\n\\n\\ndef pull_articles_from_source(url, source, article_data=[]):\\n    paper = newspaper.build(url)\\n    i = 0\\n    failed = 0\\n    print(len(paper.articles))\\n    paper.download_articles()\\n    paper.parse_articles()  # remove articles that are too small (probably not articles)\\n    print(len(paper.articles))\\n    for article in paper.articles:\\n        i += 1\\n        if i > 10:\\n            break\\n        try:\\n            # fail if the article is empty or less than 40 words\\n            if article.text.isspace() or article.text == '' or len(article.text.split(' ')) < 40:\\n                failed += 1\\n                continue\\n            article.nlp()\\n\\n            authors = article.authors\\n            temp = []\\n            for i in authors:\\n                if len(i.split(' ')) > 5:\\n                    continue\\n                temp.append(i)\\n            authors = temp\\n\\n            data = {'source': source, 'title': article.title, 'authors': authors, 'text': article.text,\\n                    'keywords': article.keywords, 'summary': article.summary, 'url': article.url,\\n                    'date': article.publish_date}\\n            article_data.append(data)\\n        except:\\n            failed += 1\\n\\n    return article_data\\n\\n\\ndef source_from_url(link):\\n\\n    if 'www' in link:\\n        source = link.split('.')[1]\\n    else:\\n        if '.com' in link:\\n            source = link.split('.com')[0]\\n        else:\\n            source = link.split('.')[0]\\n    source = source.replace('https://', '')\\n    source = source.replace('http://', '')\\n    return source\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df72789-1c36-453a-97c7-a3d7fb4bf43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
